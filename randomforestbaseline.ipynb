{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"competition","sourceId":86518,"databundleVersionId":9809560},{"sourceType":"modelInstanceVersion","sourceId":441532,"databundleVersionId":12759937,"modelInstanceId":358989},{"sourceType":"modelInstanceVersion","sourceId":440122,"databundleVersionId":12752348,"modelInstanceId":358558},{"sourceType":"modelInstanceVersion","sourceId":440119,"databundleVersionId":12752339,"modelInstanceId":358556},{"sourceType":"modelInstanceVersion","sourceId":440123,"databundleVersionId":12752351,"modelInstanceId":358559}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Random Forest Baseline","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_path = \"/kaggle/input/llm-classification-finetuning/\"\ntrain_file = os.path.join(data_path, \"train.csv\")\ntest_file = os.path.join(data_path, \"test.csv\")\nsubmission_template_file = os.path.join(data_path, \"sample_submission.csv\")\n\ntrain = pd.read_csv(train_file)\ntest = pd.read_csv(test_file)\nsubmission_template = pd.read_csv(submission_template_file)\ntrain['target'] = train[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1)\ntarget_encoder = LabelEncoder()\ntrain['target_encoded'] = target_encoder.fit_transform(train['target'])\nX = train[['response_a', 'response_b', 'prompt']]\ny = train['target_encoded']\n\ndef create_features(X):\n    X['a_length'] = X['response_a'].str.len()\n    X['b_length'] = X['response_b'].str.len()\n    X['prompt_length'] = X['prompt'].str.len()\n    X['a_words'] = X['response_a'].apply(lambda x: len(x.split()))\n    X['b_words'] = X['response_b'].apply(lambda x: len(x.split()))\n    X['prompt_words'] = X['prompt'].apply(lambda x: len(x.split()))\n    return X\n\nX = create_features(X)\nX = X.drop(['response_a', 'response_b', 'prompt'], axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\ny_val_pred = model.predict_proba(X_val)\nlogloss = log_loss(y_val, y_val_pred)\nprint(f\"Validation Log Loss: {logloss}\")\n\ntest = create_features(test)\ntest_X = test.drop(['response_a', 'response_b', 'prompt', 'id'], axis=1)\npredictions = model.predict_proba(test_X)\n\nsubmission = pd.DataFrame({\n    \"id\": test['id'],\n    \"winner_model_a\": predictions[:, 0],\n    \"winner_model_b\": predictions[:, 1],\n    \"winner_tie\": predictions[:, 2]\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main model","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    DebertaV2ForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    get_linear_schedule_with_warmup\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:50:22.150621Z","iopub.execute_input":"2025-06-19T15:50:22.150905Z","iopub.status.idle":"2025-06-19T15:50:30.801427Z","shell.execute_reply.started":"2025-06-19T15:50:22.150883Z","shell.execute_reply":"2025-06-19T15:50:30.800663Z"}},"outputs":[{"name":"stderr","text":"2025-06-19 15:50:27.736449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750348227.762676     332 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750348227.770459     332 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/debertav3_tokenizer/transformers/default/1/debertav3_tokenizer\")\nSPECIAL_TOKENS = {\n    \"sep_token\": \"[SEP]\",\n    \"response_a_token\": \"[RESP_A]\",\n    \"response_b_token\": \"[RESP_B]\"\n}\ntokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:50:31.876948Z","iopub.execute_input":"2025-06-19T15:50:31.877904Z","iopub.status.idle":"2025-06-19T15:50:32.194712Z","shell.execute_reply.started":"2025-06-19T15:50:31.877875Z","shell.execute_reply":"2025-06-19T15:50:32.193953Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model = DebertaV2ForSequenceClassification.from_pretrained(\n    \"/kaggle/input/debertav3_model/transformers/default/1/debertav3_model\",\n    num_labels=3,\n    problem_type=\"multi_label_classification\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:50:35.025164Z","iopub.execute_input":"2025-06-19T15:50:35.026266Z","iopub.status.idle":"2025-06-19T15:50:35.132096Z","shell.execute_reply.started":"2025-06-19T15:50:35.026238Z","shell.execute_reply":"2025-06-19T15:50:35.131515Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:50:37.206903Z","iopub.execute_input":"2025-06-19T15:50:37.207602Z","iopub.status.idle":"2025-06-19T15:50:38.124693Z","shell.execute_reply.started":"2025-06-19T15:50:37.207581Z","shell.execute_reply":"2025-06-19T15:50:38.123885Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Embedding(128003, 768, padding_idx=0)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"data_path = \"/kaggle/input/llm-classification-finetuning/\"\ntrain_file = os.path.join(data_path, \"train.csv\")\ntest_file = os.path.join(data_path, \"test.csv\")\nsubmission_template_file = os.path.join(data_path, \"sample_submission.csv\")\n\ntrain = pd.read_csv(train_file)\ntest = pd.read_csv(test_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:50:41.467148Z","iopub.execute_input":"2025-06-19T15:50:41.467427Z","iopub.status.idle":"2025-06-19T15:50:43.266960Z","shell.execute_reply.started":"2025-06-19T15:50:41.467409Z","shell.execute_reply":"2025-06-19T15:50:43.266115Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train, val = train_test_split(train, test_size=0.33, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:50:44.996448Z","iopub.execute_input":"2025-06-19T15:50:44.996718Z","iopub.status.idle":"2025-06-19T15:50:45.019632Z","shell.execute_reply.started":"2025-06-19T15:50:44.996698Z","shell.execute_reply":"2025-06-19T15:50:45.018814Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset_train = Dataset.from_pandas(train)\ndataset_val = Dataset.from_pandas(val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:50:47.191666Z","iopub.execute_input":"2025-06-19T15:50:47.191979Z","iopub.status.idle":"2025-06-19T15:50:48.731363Z","shell.execute_reply.started":"2025-06-19T15:50:47.191958Z","shell.execute_reply":"2025-06-19T15:50:48.730575Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocess_function(examples):\n    # Structure: [PROMPT] [SEP] [RESP_A] response_a [SEP] [RESP_B] response_b [SEP]\n    texts = [\n        f\"{p} {tokenizer.sep_token} {SPECIAL_TOKENS['response_a_token']} {a} \"\n        f\"{tokenizer.sep_token} {SPECIAL_TOKENS['response_b_token']} {b} {tokenizer.sep_token}\"\n        for p, a, b in zip(examples['prompt'], examples['response_a'], examples['response_b'])\n    ]\n    \n    tokenized = tokenizer(\n        texts,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\"\n    )\n    \n    labels = [\n        [a, b, tie] \n        for a, b, tie in zip(examples['winner_model_a'], examples['winner_model_b'], examples['winner_tie'])\n    ]\n    \n    return {\n        'input_ids': tokenized['input_ids'],\n        'attention_mask': tokenized['attention_mask'],\n        'labels': labels\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:50:49.050687Z","iopub.execute_input":"2025-06-19T15:50:49.050996Z","iopub.status.idle":"2025-06-19T15:50:49.056396Z","shell.execute_reply.started":"2025-06-19T15:50:49.050974Z","shell.execute_reply":"2025-06-19T15:50:49.055556Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tokenized_dataset_train = dataset_train.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset_train.column_names\n)\ntokenized_dataset_val = dataset_val.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset_val.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:51:34.346435Z","iopub.execute_input":"2025-06-19T15:51:34.346788Z","iopub.status.idle":"2025-06-19T15:52:39.053894Z","shell.execute_reply.started":"2025-06-19T15:51:34.346763Z","shell.execute_reply":"2025-06-19T15:52:39.053306Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/38509 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f80466b284804efcb057c69d502f3ceb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18968 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c1eb17e8f6a499e9a7e3f612dc24f01"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"lora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    bias=\"none\",\n    target_modules=[\"query_proj\", \"value_proj\"], \n    inference_mode=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:53:59.672538Z","iopub.execute_input":"2025-06-19T15:53:59.672853Z","iopub.status.idle":"2025-06-19T15:53:59.677088Z","shell.execute_reply.started":"2025-06-19T15:53:59.672817Z","shell.execute_reply":"2025-06-19T15:53:59.676267Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model_lora = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:54:01.702133Z","iopub.execute_input":"2025-06-19T15:54:01.702860Z","iopub.status.idle":"2025-06-19T15:54:01.737165Z","shell.execute_reply.started":"2025-06-19T15:54:01.702819Z","shell.execute_reply":"2025-06-19T15:54:01.736644Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model_lora.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:54:03.575649Z","iopub.execute_input":"2025-06-19T15:54:03.575953Z","iopub.status.idle":"2025-06-19T15:54:03.581345Z","shell.execute_reply.started":"2025-06-19T15:54:03.575932Z","shell.execute_reply":"2025-06-19T15:54:03.580581Z"}},"outputs":[{"name":"stdout","text":"trainable params: 297,219 || all params: 184,647,174 || trainable%: 0.1610\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"model_lora","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:54:05.213096Z","iopub.execute_input":"2025-06-19T15:54:05.213411Z","iopub.status.idle":"2025-06-19T15:54:05.221813Z","shell.execute_reply.started":"2025-06-19T15:54:05.213388Z","shell.execute_reply":"2025-06-19T15:54:05.220892Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): DebertaV2ForSequenceClassification(\n      (deberta): DebertaV2Model(\n        (embeddings): DebertaV2Embeddings(\n          (word_embeddings): Embedding(128003, 768, padding_idx=0)\n          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): DebertaV2Encoder(\n          (layer): ModuleList(\n            (0-11): 12 x DebertaV2Layer(\n              (attention): DebertaV2Attention(\n                (self): DisentangledSelfAttention(\n                  (query_proj): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n                  (value_proj): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (pos_dropout): Dropout(p=0.1, inplace=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): DebertaV2SelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): DebertaV2Intermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): DebertaV2Output(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (rel_embeddings): Embedding(512, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n        )\n      )\n      (pooler): ContextPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=768, out_features=3, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=768, out_features=3, bias=True)\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    pred_probs = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n    \n    pred_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(labels, axis=1)\n    \n    return {\n        \"accuracy\": accuracy_score(true_classes, pred_classes),\n        \"log_loss\": log_loss(true_classes, pred_probs, normalize = True),\n        \"class_distribution\": dict(zip(\n            [\"prefer_a\", \"prefer_b\", \"tie\"],\n            np.bincount(true_classes, minlength=3) / len(true_classes)\n        ))}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:54:09.095130Z","iopub.execute_input":"2025-06-19T15:54:09.095825Z","iopub.status.idle":"2025-06-19T15:54:09.100337Z","shell.execute_reply.started":"2025-06-19T15:54:09.095801Z","shell.execute_reply":"2025-06-19T15:54:09.099546Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),\n        'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),\n        'labels': torch.tensor([item['labels'] for item in batch], dtype=torch.float)\n    }\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_fct = nn.CrossEntropyLoss()\n        \n    def compute_loss(self, model, inputs,num_items_in_batch = 1, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        \n        # Convert probability labels to class indices for CE loss\n        class_labels = torch.argmax(labels, dim=1)\n        loss = self.loss_fct(logits, class_labels)\n        \n        return (loss, outputs) if return_outputs else loss\n\ntraining_args = TrainingArguments(\n    output_dir=\"./debertv3_preference_results\",\n    learning_rate=2e-5, \n    per_device_train_batch_size=8, \n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_log_loss\",\n    logging_dir='./logs',\n    logging_steps=10,\n    gradient_accumulation_steps=2,\n    fp16=True,\n    report_to=\"none\"\n)\n\n\ntrainer = CustomTrainer(\n    model=model_lora,\n    args=training_args,\n    train_dataset=tokenized_dataset_train,\n    eval_dataset=tokenized_dataset_val,\n    compute_metrics=compute_metrics,\n    data_collator=collate_fn,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:54:10.865147Z","iopub.execute_input":"2025-06-19T15:54:10.865871Z","iopub.status.idle":"2025-06-19T17:29:40.790523Z","shell.execute_reply.started":"2025-06-19T15:54:10.865819Z","shell.execute_reply":"2025-06-19T17:29:40.789894Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4814' max='4814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4814/4814 1:35:27, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Log Loss</th>\n      <th>Class Distribution</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n      <th>Steps Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.103600</td>\n      <td>1.090222</td>\n      <td>0.377478</td>\n      <td>1.090222</td>\n      <td>{'prefer_a': 0.35153943483762123, 'prefer_b': 0.34020455504006747, 'tie': 0.30825601012231124}</td>\n      <td>475.769700</td>\n      <td>39.868000</td>\n      <td>2.493000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.087500</td>\n      <td>1.085305</td>\n      <td>0.385491</td>\n      <td>1.085305</td>\n      <td>{'prefer_a': 0.35153943483762123, 'prefer_b': 0.34020455504006747, 'tie': 0.30825601012231124}</td>\n      <td>475.980200</td>\n      <td>39.850000</td>\n      <td>2.492000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4814, training_loss=1.093341258635004, metrics={'train_runtime': 5729.1466, 'train_samples_per_second': 13.443, 'train_steps_per_second': 0.84, 'total_flos': 2.0335154429251584e+16, 'train_loss': 1.093341258635004, 'epoch': 2.0})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"def predict_preference(prompt, response_a, response_b):\n    text = (\n        f\"{prompt} {tokenizer.sep_token} {SPECIAL_TOKENS['response_a_token']} {response_a} \"\n        f\"{tokenizer.sep_token} {SPECIAL_TOKENS['response_b_token']} {response_b} {tokenizer.sep_token}\"\n    )\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    probs = torch.softmax(outputs.logits, dim=-1).squeeze()\n    \n    return {\n        \"winner_model_a\": probs[0].item(),\n        \"winner_model_b\": probs[1].item(),\n        \"winner_tie\": probs[2].item(),\n        \n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = train.iloc[5][\"prompt\"]\nresponse_a = train.iloc[5][\"response_a\"]\nresponse_b = train.iloc[5][\"response_b\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_preference(prompt, response_a, response_b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T17:30:13.948706Z","iopub.execute_input":"2025-06-19T17:30:13.949291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def batch_generator(df, batch_size):\n    for i in range(0, len(df), batch_size):\n        yield df.iloc[i:i + batch_size]\n\ndef generate_submission_csv(\n    test_csv_path: str,\n    output_csv_path: str = \"submission.csv\",\n    batch_size: int = 8,\n    max_length: int = 512\n) -> None:\n   \n    test_df = pd.read_csv(test_csv_path)\n    predictions = []\n    for batch in tqdm(batch_generator(test_df, batch_size), desc=\"Processing batches\"):\n        texts = [\n            f\"{row.prompt} [SEP] [RESP_A] {row.response_a} [SEP] [RESP_B] {row.response_b} [SEP]\"\n            for _, row in batch.iterrows()\n        ]\n\n        inputs = tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=max_length,\n            return_tensors=\"pt\"\n        )\n        \n        if torch.cuda.is_available():\n            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n            batch_probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n        \n        predictions.extend(batch_probs)\n    \n    submission = pd.DataFrame({\n        \"id\": list(test_df['id'].values),\n        \"winner_model_a\": [p[0] for p in predictions],\n        \"winner_model_b\": [p[1] for p in predictions],\n        \"winner_tie\": [p[2] for p in predictions]\n    })\n    \n\n    submission.to_csv(output_csv_path, index=False)\n    print(f\"Submission saved to {output_csv_path}\")\n\ngenerate_submission_csv(\n    test_csv_path=\"/kaggle/input/llm-classification-finetuning/test.csv\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}